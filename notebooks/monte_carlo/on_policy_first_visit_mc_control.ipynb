{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c004388",
   "metadata": {},
   "source": [
    "# On-Policy First-Visit MC Control (for $\\epsilon$-soft policies), estimates $\\pi\\approx\\pi_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec43256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e07e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3142b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_greedy_policy(Q):\n",
    "    def policy(state):\n",
    "        return np.argmax(Q[state])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049d7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy(state):\n",
    "        action_probs = np.ones(nA) * (epsilon / nA)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        action_probs[best_action] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776b4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, max_steps=100):\n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        if not done:\n",
    "            reward -= 0.01  # Step penalty\n",
    "        elif reward == 0:\n",
    "            reward = -1.0   # Hole penalty\n",
    "        else:\n",
    "            reward = 5.0    # Reaching goal\n",
    "\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a890d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_mc_control(env, epsilon_start, num_episodes, gamma):\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    returns_count = np.zeros((nS, nA))\n",
    "\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9993  # Slower decay keeps more exploration\n",
    "\n",
    "    for i in tqdm(range(num_episodes), desc=\"Training episodes\"):\n",
    "        epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** i))\n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, nA)\n",
    "        episode = generate_episode(env, policy)\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns_count[state][action] += 1\n",
    "                Q[state][action] += (1 / returns_count[state][action]) * (G - Q[state][action])\n",
    "\n",
    "    return make_greedy_policy(Q), Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2c6ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_map = [\n",
    "    \"SFFHF\",\n",
    "    \"HFHFF\",\n",
    "    \"FFFHF\",\n",
    "    \"FHFFF\",\n",
    "    \"HFFFG\"\n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "n_episodes = 3000\n",
    "gamma = 0.98\n",
    "epsilon_start = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc926d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:01<00:00, 1505.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train environment\n",
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=False)\n",
    "policy, Q = on_policy_first_visit_mc_control(env, epsilon_start, n_episodes, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Q-table for later use\n",
    "np.savez('results/frozenlake-qtable.npz', Q=Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e109d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=False, render_mode=\"human\")\n",
    "state = env.reset()[0] \n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncate, info = env.step(action)\n",
    "    time.sleep(0.3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf056cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m env.close()\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# âœ… Save as looping GIF (loop=0 means infinite loop)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m images = [\u001b[43mImage\u001b[49m.fromarray(frame) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames]\n\u001b[32m     24\u001b[39m images[\u001b[32m0\u001b[39m].save(\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfrozenlake_agent.gif\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     save_all=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     loop=\u001b[32m0\u001b[39m\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGIF saved as frozenlake_agent.gif\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# ðŸŽ¥ Re-run environment to record a successful episode using greedy policy\n",
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=False, render_mode=\"rgb_array\")\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "\n",
    "frames = []\n",
    "max_steps = 100  # To prevent infinite loops in case of error\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        frames.append(env.render())  # Final frame (goal or hole)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# âœ… Save as looping GIF (loop=0 means infinite loop)\n",
    "images = [Image.fromarray(frame) for frame in frames]\n",
    "images[0].save(\n",
    "    'frozenlake_agent.gif',\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    duration=100,  # Adjust speed\n",
    "    loop=0\n",
    ")\n",
    "\n",
    "print(\"GIF saved as frozenlake_agent.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88b82d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a GIF of the agent following the greedy policy\n",
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=False, render_mode=\"rgb_array\")\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "\n",
    "frames = []\n",
    "while not done:\n",
    "    frames.append(env.render())\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncate, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert frames to a GIF\n",
    "images = [Image.fromarray(frame) for frame in frames]\n",
    "images[0].save('frozenlake_agent.gif', save_all=True, append_images=images[:], duration=100, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd6d8bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as frozenlake_agent.gif\n"
     ]
    }
   ],
   "source": [
    "# ðŸŽ¥ Re-run environment to record a successful episode using greedy policy\n",
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map_5x5, is_slippery=False, render_mode=\"rgb_array\")\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "\n",
    "frames = []\n",
    "max_steps = 100  # To prevent infinite loops in case of error\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        frames.append(env.render())  # Final frame (goal or hole)\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# âœ… Save as looping GIF (loop=0 means infinite loop)\n",
    "images = [Image.fromarray(frame) for frame in frames]\n",
    "images[0].save(\n",
    "    'frozenlake_agent.gif',\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    duration=400,  # Adjust speed\n",
    "    loop=0\n",
    ")\n",
    "\n",
    "print(\"GIF saved as frozenlake_agent.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8ea917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b1cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_greedy_policy(Q, nA):\n",
    "    def policy(state):\n",
    "        best_action = max(range(nA), key=lambda a: Q[(state, a)])\n",
    "        return best_action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9e4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy(state):\n",
    "        action_probs = np.ones(nA) * (epsilon / nA)\n",
    "        best_action = max(range(nA), key=lambda a: Q[(state, a)])\n",
    "        action_probs[best_action] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc127ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "\n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_mc_control(env, epsilon, num_episodes, gamma):\n",
    "\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    returns_count = np.zeros((nS, nA))\n",
    "\n",
    "    for i in tqdm(range(num_episodes), desc=\"Training episodes\"):\n",
    "\n",
    "        # epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** i))\n",
    "        epsilon = max(0.01, epsilon * 0.9999) # annealing epsilon\n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, nA)\n",
    "\n",
    "        episode = generate_episode(env, policy)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "\n",
    "        for t in reversed(range(len(episode))):\n",
    "\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns_count[state][action] += 1\n",
    "                Q[state][action] += (1/returns_count[state][action]) * (G - Q[state][action])\n",
    "\n",
    "    return make_greedy_policy(Q, nA), Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64860210",
   "metadata": {},
   "source": [
    "## FrozenLake-v1 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "621e4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:01<00:00, 1500.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def make_greedy_policy(Q, nA):\n",
    "    def policy(state):\n",
    "        return np.argmax(Q[state])\n",
    "    return policy\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy(state):\n",
    "        action_probs = np.ones(nA) * (epsilon / nA)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        action_probs[best_action] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "    return policy\n",
    "\n",
    "def generate_episode(env, policy, max_steps=100):\n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        if not done:\n",
    "            reward -= 0.01  # Step penalty\n",
    "        elif reward == 0:\n",
    "            reward = -1.0   # Hole penalty\n",
    "        else:\n",
    "            reward = 5.0    # Reaching goal\n",
    "\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    return episode\n",
    "\n",
    "def on_policy_first_visit_mc_control(env, epsilon_start, num_episodes, gamma):\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    returns_count = np.zeros((nS, nA))\n",
    "\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9993  # Slower decay keeps more exploration\n",
    "\n",
    "    for i in tqdm(range(num_episodes), desc=\"Training episodes\"):\n",
    "        epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** i))\n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, nA)\n",
    "        episode = generate_episode(env, policy)\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns_count[state][action] += 1\n",
    "                Q[state][action] += (1 / returns_count[state][action]) * (G - Q[state][action])\n",
    "\n",
    "    return make_greedy_policy(Q, nA), Q\n",
    "\n",
    "# ðŸŒŸ Use the 5x5 custom map\n",
    "custom_map_5x5 = [\n",
    "    \"SFFHF\",\n",
    "    \"HFHFF\",\n",
    "    \"FFFHF\",\n",
    "    \"FHFFF\",\n",
    "    \"HFFFG\"\n",
    "]\n",
    "\n",
    "# ðŸš€ Hyperparameters\n",
    "n_episodes = 3000  # Lowered for speed + good enough convergence\n",
    "gamma = 0.98\n",
    "epsilon_start = 1.0\n",
    "\n",
    "# ðŸŽ® Train environment\n",
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map_5x5, is_slippery=False)\n",
    "policy, Q = on_policy_first_visit_mc_control(env, epsilon_start, n_episodes, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b15a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map_5x5, is_slippery=False, render_mode=\"human\")\n",
    "state = env.reset()[0] \n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncate, info = env.step(action)\n",
    "    time.sleep(0.3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d86760",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "819b4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [00:25<00:00, 1190.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def make_greedy_policy(Q, nA):\n",
    "    def policy(state):\n",
    "        return np.argmax(Q[state])\n",
    "    return policy\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy(state):\n",
    "        action_probs = np.ones(nA) * (epsilon / nA)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        action_probs[best_action] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "    return policy\n",
    "\n",
    "def generate_episode(env, policy, max_steps=100):\n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    return episode\n",
    "\n",
    "def on_policy_first_visit_mc_control(env, epsilon_start, num_episodes, gamma):\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    returns_count = np.zeros((nS, nA))\n",
    "\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9993\n",
    "\n",
    "    for i in tqdm(range(num_episodes), desc=\"Training episodes\"):\n",
    "        epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** i))\n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, nA)\n",
    "        episode = generate_episode(env, policy)\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns_count[state][action] += 1\n",
    "                Q[state][action] += (1 / returns_count[state][action]) * (G - Q[state][action])\n",
    "\n",
    "    return make_greedy_policy(Q, nA), Q\n",
    "\n",
    "# Hyperparameters\n",
    "n_episodes = 30000\n",
    "gamma = 0.98\n",
    "epsilon_start = 1.0\n",
    "\n",
    "# Use default FrozenLake 4x4 environment (slippery by default)\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "\n",
    "policy, Q = on_policy_first_visit_mc_control(env, epsilon_start, n_episodes, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdcb15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    time.sleep(0.3)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad36b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ae656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_greedy_policy(Q, nA):\n",
    "    def policy(state):\n",
    "        best_action = max(range(nA), key=lambda a: Q[(state, a)])\n",
    "        return best_action\n",
    "    return policy\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy(state):\n",
    "        action_probs = np.ones(nA) * (epsilon / nA)\n",
    "        best_action = max(range(nA), key=lambda a: Q[(state, a)])\n",
    "        action_probs[best_action] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "    return policy\n",
    "\n",
    "def generate_episode(env, policy):\n",
    "    episode = []\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        if not done:\n",
    "            reward -= 0.01 \n",
    "        elif reward == 0:\n",
    "            reward = -1.0 \n",
    "        else:\n",
    "            reward = +5.0\n",
    "\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "def on_policy_first_visit_mc_control(env, epsilon_start, num_episodes, gamma):\n",
    "\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    returns_count = np.zeros((nS, nA))\n",
    "    \n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.9995\n",
    "    \n",
    "    for i in tqdm(range(num_episodes), desc=\"Training episodes\"):\n",
    " \n",
    "        epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** i))\n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, nA)\n",
    "        episode = generate_episode(env, policy)\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns_count[state][action] += 1\n",
    "                Q[state][action] += (1/returns_count[state][action]) * (G - Q[state][action])\n",
    "                \n",
    "    return make_greedy_policy(Q, nA), Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26568281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training episodes:   9%|â–‰         | 8951/100000 [04:55<50:03, 30.31it/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m epsilon_start = \u001b[32m1.0\u001b[39m\n\u001b[32m     16\u001b[39m env = gym.make(\u001b[33m\"\u001b[39m\u001b[33mFrozenLake-v1\u001b[39m\u001b[33m\"\u001b[39m, desc=custom_map, is_slippery=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m policy, Q = \u001b[43mon_policy_first_visit_mc_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mon_policy_first_visit_mc_control\u001b[39m\u001b[34m(env, epsilon_start, num_episodes, gamma)\u001b[39m\n\u001b[32m     55\u001b[39m epsilon = \u001b[38;5;28mmax\u001b[39m(epsilon_min, epsilon_start * (epsilon_decay ** i))\n\u001b[32m     57\u001b[39m policy = make_epsilon_greedy_policy(Q, epsilon, nA)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m episode = \u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m G = \u001b[32m0\u001b[39m\n\u001b[32m     60\u001b[39m visited = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mgenerate_episode\u001b[39m\u001b[34m(env, policy)\u001b[39m\n\u001b[32m     32\u001b[39m action_probs = policy(state)\n\u001b[32m     33\u001b[39m action = np.random.choice(\u001b[38;5;28mlen\u001b[39m(action_probs), p=action_probs)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m next_state, reward, done, _, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Add small negative step penalty to encourage shorter paths\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:307\u001b[39m, in \u001b[36mFrozenLakeEnv.step\u001b[39m\u001b[34m(self, a)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[32m    306\u001b[39m     transitions = \u001b[38;5;28mself\u001b[39m.P[\u001b[38;5;28mself\u001b[39m.s][a]\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     i = \u001b[43mcategorical_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnp_random\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m     p, s, r, t = transitions[i]\n\u001b[32m    309\u001b[39m     \u001b[38;5;28mself\u001b[39m.s = s\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/utils.py:8\u001b[39m, in \u001b[36mcategorical_sample\u001b[39m\u001b[34m(prob_n, np_random)\u001b[39m\n\u001b[32m      6\u001b[39m prob_n = np.asarray(prob_n)\n\u001b[32m      7\u001b[39m csprob_n = np.cumsum(prob_n)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsprob_n\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_random\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:1342\u001b[39m, in \u001b[36margmax\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1253\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1254\u001b[39m \u001b[33;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[32m   1255\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1339\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1340\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1341\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1342\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/f/Projects/RL/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:52\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapfunc\u001b[39m(obj, method, *args, **kwds):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     bound = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "custom_map = [\n",
    "    \"SFFFFH\",\n",
    "    \"HFHFHF\",\n",
    "    \"FFFHFF\",\n",
    "    \"FHFHFF\",\n",
    "    \"FFHFFF\",\n",
    "    \"HFFFHG\"\n",
    "]\n",
    "\n",
    "\n",
    "# Good hyperparameters for custom 6x6 map\n",
    "n_episodes = 100000  # More episodes for larger state space\n",
    "gamma = 0.95\n",
    "epsilon_start = 1.0\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=False)\n",
    "policy, Q = on_policy_first_visit_mc_control(env, epsilon_start, n_episodes, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56e1cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=False, render_mode=\"human\")\n",
    "state = env.reset()[0] \n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncate, info = env.step(action)\n",
    "    time.sleep(0.3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d27160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:20<00:00, 2487.53it/s]\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 50000\n",
    "gamma = 0.95\n",
    "epsilon_start = 1.0\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "policy, Q = on_policy_first_visit_mc_control(env, epsilon_start, n_episodes, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30a1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "state = env.reset()[0] \n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncate, info = env.step(action)\n",
    "    time.sleep(0.3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60aed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"results/policy_iteration.npz\", Q=Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e6196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced8866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378fa80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e2c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133f382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba49e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41431d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "317188b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500000/500000 [06:51<00:00, 1215.94it/s]\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 500000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_decay = 0.0001\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=False)\n",
    "policy, Q = on_policy_first_visit_mc_control(env, 1, n_episodes, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23536964",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "state = env.reset()[0] \n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done and step < 25:\n",
    "    env.render()\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncate, info = env.step(action)\n",
    "    step += 1\n",
    "    time.sleep(0.3)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8739a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500000/500000 [38:15<00:00, 217.85it/s]  \n"
     ]
    }
   ],
   "source": [
    "n_episodes = 500000\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_decay = 0.0001\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True)\n",
    "policy, Q = on_policy_first_visit_mc_control(env, 1, n_episodes, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d08cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "state = env.reset()[0] \n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done and step < 200:\n",
    "    env.render()\n",
    "    action = policy(state)\n",
    "    state, reward, done, truncate, info = env.step(action)\n",
    "    step += 1\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd066305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "783ab07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random numbers with seed 42:\n",
      "0.3745401188473625\n",
      "0.9507143064099162\n",
      "0.7319939418114051\n",
      "0.5986584841970366\n",
      "0.15601864044243652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Random numbers with seed 42:\")\n",
    "for _ in range(5):\n",
    "    print(np.random.rand())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4281ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random numbers without seed:\n",
      "0.6118528947223795\n",
      "0.13949386065204183\n",
      "0.29214464853521815\n",
      "0.3663618432936917\n",
      "0.45606998421703593\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "print(\"Random numbers without seed:\")\n",
    "for _ in range(5):\n",
    "    print(np.random.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "519724ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507143064099162"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "np.random.rand()\n",
    "np.random.rand()\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "np.random.rand()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16d8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
